{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform ensemble evaluation from logits files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff and create arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import model_selector\n",
    "from utils.data_augmentation import data_augmentation_selector\n",
    "from utils.datasets import dataset_selector\n",
    "from utils.neural import *\n",
    "from utils.metrics import compute_accuracy\n",
    "from utils.calibration import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_dir=\"../logits\"\n",
    "prefix = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_logits_paths = []\n",
    "for subdir, dirs, files in os.walk(logits_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        if f\"{prefix}_logits\" in file_path:\n",
    "            val_logits_paths.append(file_path)\n",
    "\n",
    "if not len(val_logits_paths):\n",
    "    assert False, f\"Could not find any file at subdirectoreis of '{logits_dir}' with prefix '{prefix}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_dir=\"../logits\"\n",
    "prefix = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits_paths = []\n",
    "for subdir, dirs, files in os.walk(logits_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        if f\"{prefix}_logits\" in file_path:\n",
    "            test_logits_paths.append(file_path)\n",
    "\n",
    "if not len(test_logits_paths):\n",
    "    assert False, f\"Could not find any file at subdirectories of '{logits_dir}' with prefix '{prefix}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../logits/model1/val_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model2/val_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model3/val_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model4/val_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model5/val_logits_model_kuangliu_resnet18_best_accuracy.pt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_logits_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../logits/model1/test_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model2/test_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model3/test_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model4/test_logits_model_kuangliu_resnet18_best_accuracy.pt',\n",
       " '../logits/model5/test_logits_model_kuangliu_resnet18_best_accuracy.pt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logits_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get logits and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1/val_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9478\n",
      "model2/val_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9488\n",
      "model3/val_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.95\n",
      "model4/val_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9472\n",
      "model5/val_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.949\n"
     ]
    }
   ],
   "source": [
    "val_logits_list, val_labels_list = [], []\n",
    "for lp in val_logits_paths:\n",
    "    logits_name = \"/\".join(lp.split(\"/\")[-2:])\n",
    "    info = torch.load(lp)\n",
    "    logits = info[\"logits\"].cpu()\n",
    "    labels = info[\"labels\"].cpu()\n",
    "\n",
    "    logits_accuracy = compute_accuracy(labels, logits)\n",
    "    print(f\"{logits_name}: {logits_accuracy}\")\n",
    "    val_logits_list.append(logits)\n",
    "    val_labels_list.append(labels)\n",
    "\n",
    "# logits_list shape: torch.Size([N, 10000, 10]) (CIFAR10 example)\n",
    "val_logits_list = torch.stack(val_logits_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Check if al labels has the same order for all logits --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val_labels_list[0]\n",
    "for indx, label_list in enumerate(val_labels_list[1:]):\n",
    "    # Si alguno difiere del primero es que no es igual al resto tampoco\n",
    "    if not torch.all(labels.eq(label_list)):\n",
    "        assert False, f\"Labels list does not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1/test_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9482\n",
      "model2/test_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9438\n",
      "model3/test_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9469\n",
      "model4/test_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9448\n",
      "model5/test_logits_model_kuangliu_resnet18_best_accuracy.pt: 0.9458\n"
     ]
    }
   ],
   "source": [
    "test_logits_list, test_labels_list = [], []\n",
    "for lp in test_logits_paths:\n",
    "    logits_name = \"/\".join(lp.split(\"/\")[-2:])\n",
    "    info = torch.load(lp)\n",
    "    logits = info[\"logits\"].cpu()\n",
    "    labels = info[\"labels\"].cpu()\n",
    "\n",
    "    logits_accuracy = compute_accuracy(labels, logits)\n",
    "    print(f\"{logits_name}: {logits_accuracy}\")\n",
    "    test_logits_list.append(logits)\n",
    "    test_labels_list.append(labels)\n",
    "\n",
    "# logits_list shape: torch.Size([N, 10000, 10]) (CIFAR10 example)\n",
    "test_logits_list = torch.stack(test_logits_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Check if al labels has the same order for all logits --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels_list[0]\n",
    "for indx, label_list in enumerate(test_labels_list[1:]):\n",
    "    # Si alguno difiere del primero es que no es igual al resto tampoco\n",
    "    if not torch.all(labels.eq(label_list)):\n",
    "        assert False, f\"Labels list does not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling\n",
    "https://github.com/gpleiss/temperature_scaling/blob/master/temperature_scaling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempScaling(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits, NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TempScaling, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return logits * temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train temperature parameter over first validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models logits: torch.Size([5, 5000, 10])\n",
      "First model logits: torch.Size([5000, 10])\n",
      "All labels are shared by validation sets: torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "val_first_model_logits = val_logits_list[0]\n",
    "print(f\"All models logits: {val_logits_list.shape}\")\n",
    "print(f\"First model logits: {val_first_model_logits.shape}\")\n",
    "print(f\"All labels are shared by validation sets: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "T1 = TempScaling()\n",
    "optimizer = torch.optim.SGD(T1.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(0,100,20), gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________\n",
      "|  Epoch   |    LR    |   Loss   | Temp Param | Accuracy |  ECE   |  MCE   | BRIER  |  NNL   |\n",
      "______________________________________________________________________________________________\n",
      "|    1     | 0.010000 |  0.1931  |   0.7663   |  0.9478  | 4.5543 | 1.3636 | 0.0087 | 0.1931 |\n",
      "|    2     | 0.010000 |  0.1928  |   0.7627   |  0.9478  | 4.4926 | 1.3238 | 0.0087 | 0.1928 |\n",
      "|    3     | 0.010000 |  0.1928  |   0.7634   |  0.9478  | 4.4716 | 1.3390 | 0.0087 | 0.1928 |\n",
      "|    4     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3241 | 0.0087 | 0.1928 |\n",
      "|    5     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    6     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    7     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    8     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    9     | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    10    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    11    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    12    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    13    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    14    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    15    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    16    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    17    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    18    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    19    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    20    | 0.010000 |  0.1928  |   0.7633   |  0.9478  | 4.4647 | 1.3240 | 0.0087 | 0.1928 |\n",
      "|    21    | 0.001000 |  0.1924  |   0.7571   |  0.9478  | 4.2535 | 1.3076 | 0.0087 | 0.1924 |\n",
      "|    22    | 0.001000 |  0.1923  |   0.7522   |  0.9478  | 4.2924 | 1.2929 | 0.0087 | 0.1923 |\n",
      "|    23    | 0.001000 |  0.1923  |   0.7489   |  0.9478  | 4.2261 | 1.2820 | 0.0087 | 0.1923 |\n",
      "|    24    | 0.001000 |  0.1923  |   0.7468   |  0.9478  | 4.2118 | 1.2875 | 0.0087 | 0.1923 |\n",
      "|    25    | 0.001000 |  0.1923  |   0.7454   |  0.9478  | 4.2317 | 1.2888 | 0.0087 | 0.1923 |\n",
      "|    26    | 0.001000 |  0.1923  |   0.7445   |  0.9478  | 4.2654 | 1.3048 | 0.0087 | 0.1923 |\n",
      "|    27    | 0.001000 |  0.1923  |   0.7439   |  0.9478  | 4.2970 | 1.3181 | 0.0087 | 0.1923 |\n",
      "|    28    | 0.001000 |  0.1923  |   0.7435   |  0.9478  | 4.2958 | 1.3181 | 0.0087 | 0.1923 |\n",
      "|    29    | 0.001000 |  0.1923  |   0.7433   |  0.9478  | 4.2894 | 1.3169 | 0.0087 | 0.1923 |\n",
      "|    30    | 0.001000 |  0.1923  |   0.7431   |  0.9478  | 4.2875 | 1.3156 | 0.0087 | 0.1923 |\n",
      "|    31    | 0.001000 |  0.1923  |   0.7430   |  0.9478  | 4.2879 | 1.3156 | 0.0087 | 0.1923 |\n",
      "|    32    | 0.001000 |  0.1923  |   0.7429   |  0.9478  | 4.2855 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    33    | 0.001000 |  0.1923  |   0.7429   |  0.9478  | 4.2857 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    34    | 0.001000 |  0.1923  |   0.7429   |  0.9478  | 4.2858 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    35    | 0.001000 |  0.1923  |   0.7429   |  0.9478  | 4.3145 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    36    | 0.001000 |  0.1923  |   0.7428   |  0.9478  | 4.3146 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    37    | 0.001000 |  0.1923  |   0.7428   |  0.9478  | 4.3146 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    38    | 0.001000 |  0.1923  |   0.7428   |  0.9478  | 4.3146 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    39    | 0.001000 |  0.1923  |   0.7428   |  0.9478  | 4.3146 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    40    | 0.001000 |  0.1923  |   0.7428   |  0.9478  | 4.3146 | 1.3144 | 0.0087 | 0.1923 |\n",
      "|    41    | 0.000100 |  0.1922  |   0.7428   |  0.9478  | 4.2486 | 1.2897 | 0.0087 | 0.1922 |\n",
      "|    42    | 0.000100 |  0.1922  |   0.7427   |  0.9478  | 4.2488 | 1.2897 | 0.0087 | 0.1922 |\n",
      "|    43    | 0.000100 |  0.1922  |   0.7426   |  0.9478  | 4.2489 | 1.2910 | 0.0087 | 0.1922 |\n",
      "|    44    | 0.000100 |  0.1922  |   0.7426   |  0.9478  | 4.2491 | 1.2910 | 0.0087 | 0.1922 |\n",
      "|    45    | 0.000100 |  0.1922  |   0.7425   |  0.9478  | 4.2821 | 1.2997 | 0.0087 | 0.1922 |\n",
      "|    46    | 0.000100 |  0.1922  |   0.7425   |  0.9478  | 4.2775 | 1.2997 | 0.0087 | 0.1922 |\n",
      "|    47    | 0.000100 |  0.1922  |   0.7424   |  0.9478  | 4.2776 | 1.2998 | 0.0087 | 0.1922 |\n",
      "|    48    | 0.000100 |  0.1922  |   0.7424   |  0.9478  | 4.2778 | 1.2998 | 0.0087 | 0.1922 |\n",
      "|    49    | 0.000100 |  0.1922  |   0.7423   |  0.9478  | 4.2779 | 1.2998 | 0.0087 | 0.1922 |\n",
      "|    50    | 0.000100 |  0.1922  |   0.7423   |  0.9478  | 4.2781 | 1.2998 | 0.0087 | 0.1922 |\n",
      "|    51    | 0.000100 |  0.1922  |   0.7422   |  0.9478  | 4.2782 | 1.2998 | 0.0087 | 0.1922 |\n",
      "|    52    | 0.000100 |  0.1922  |   0.7422   |  0.9478  | 4.2783 | 1.3141 | 0.0087 | 0.1922 |\n",
      "|    53    | 0.000100 |  0.1922  |   0.7421   |  0.9478  | 4.2759 | 1.3128 | 0.0087 | 0.1922 |\n",
      "|    54    | 0.000100 |  0.1922  |   0.7421   |  0.9478  | 4.2760 | 1.3128 | 0.0087 | 0.1922 |\n",
      "|    55    | 0.000100 |  0.1922  |   0.7421   |  0.9478  | 4.2761 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    56    | 0.000100 |  0.1922  |   0.7420   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    57    | 0.000100 |  0.1922  |   0.7420   |  0.9478  | 4.2895 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    58    | 0.000100 |  0.1922  |   0.7419   |  0.9478  | 4.2896 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    59    | 0.000100 |  0.1922  |   0.7419   |  0.9478  | 4.2897 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    60    | 0.000100 |  0.1922  |   0.7419   |  0.9478  | 4.2898 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    61    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    62    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    63    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    64    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    65    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    66    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    67    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    68    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    69    | 0.000010 |  0.1922  |   0.7419   |  0.9478  | 4.2893 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    70    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    71    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    72    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    73    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    74    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    75    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    76    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    77    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    78    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    79    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    80    | 0.000010 |  0.1922  |   0.7418   |  0.9478  | 4.2895 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    81    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    82    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    83    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    84    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    85    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    86    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    87    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    88    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    89    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    90    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    91    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    92    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    93    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    94    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    95    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    96    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    97    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    98    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|    99    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n",
      "|   100    | 0.000001 |  0.1922  |   0.7418   |  0.9478  | 4.2894 | 1.3129 | 0.0087 | 0.1922 |\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "header = \"| {:{align}{widthL}} | {:{align}{widthA}} | {:{align}{widthA}} | {:{align}{widthLL}} | {:{align}{widthA}} | {:{align}{widthM}} | {:{align}{widthM}} | {:{align}{widthM}} | {:{align}{widthM}} |\".format(\n",
    "    \"Epoch\", \"LR\", \"Loss\", \"Temp Param\", \"Accuracy\", \"ECE\", \"MCE\", \"BRIER\", \"NNL\", align='^', widthL=8, widthLL=10, widthA=8, widthM=6,\n",
    ")\n",
    "\n",
    "print(\"\".join([\"_\"] * len(header)))\n",
    "print(header)\n",
    "print(\"\".join([\"_\"]*len(header)))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    T1.train()\n",
    "    train_loss, correct, total = [], 0, 0\n",
    "    c_ece, c_mce, c_brier, c_nnl = [], [], [], []\n",
    "    \n",
    "    for c_logits, c_labels in zip(chunks(val_first_model_logits, batch_size), chunks(val_labels, batch_size)):\n",
    "\n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        new_logits = T1(c_logits)\n",
    "        loss = criterion(new_logits, c_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        train_loss.append(loss.item())\n",
    "        _, predicted = new_logits.max(1)\n",
    "        total += len(c_labels)\n",
    "        correct += predicted.eq(c_labels).sum().item()\n",
    "        \n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        new_probs_list = softmax(new_logits)\n",
    "        ece, mce, brier, nnl = compute_calibration_metrics(new_probs_list, c_labels, apply_softmax=False, bins=15)\n",
    "        c_ece.append(ece); c_mce.append(mce); c_brier.append(brier.item()); c_nnl.append(nnl.item()); \n",
    "    \n",
    "    c_train_loss = np.array(train_loss).mean()\n",
    "    c_accuracy = correct/total\n",
    "    c_ece = np.array(c_ece).mean()\n",
    "    c_mce = np.array(c_mce).mean()\n",
    "    c_brier = np.array(c_brier).mean()\n",
    "    c_nnl = np.array(c_nnl).mean()\n",
    "    current_lr = get_current_lr(optimizer)\n",
    "    \n",
    "    line = \"| {:{align}{widthL}} | {:{align}{widthA}.6f} | {:{align}{widthA}.4f} | {:{align}{widthLL}.4f} | {:{align}{widthA}.4f} | {:{align}{widthM}.4f} | {:{align}{widthM}.4f} | {:{align}{widthM}.4f} | {:{align}{widthM}.4f} |\".format(\n",
    "           epoch+1, current_lr, c_train_loss, T1.temperature.item(), c_accuracy, c_ece, c_mce, c_brier, c_nnl,\n",
    "           align='^', widthL=8, widthA=8, widthM=6, widthLL=10\n",
    "    )\n",
    "    print(line)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TempScaling()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1.eval()\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- First model Calibration Metrics over Validation ----\n",
      "ECE: 2.4941030383110045\n",
      "MCE: 1.2195143759250642\n",
      "BRIER: 0.008098339661955833\n",
      "NNL: 0.19149670004844666\n",
      "\n",
      "---- First model Calibration Metrics over Validation (Temp Scal) ----\n",
      "ECE: 0.942747556567192\n",
      "MCE: 0.1852864122390747\n",
      "BRIER: 0.007771474774926901\n",
      "NNL: 0.17561443150043488\n"
     ]
    }
   ],
   "source": [
    "new_val_first_logits = T1(val_first_model_logits)\n",
    "val_new_first_probs = softmax(new_val_first_logits)\n",
    "val_first_probs = softmax(val_first_model_logits)\n",
    "\n",
    "print(\"---- First model Calibration Metrics over Validation ----\")\n",
    "ECE, MCE, BRIER, NNL = compute_calibration_metrics(val_first_probs, val_labels, apply_softmax=False, bins=15)\n",
    "print(f\"ECE: {ECE}\"); print(f\"MCE: {MCE}\"); print(f\"BRIER: {BRIER}\"); print(f\"NNL: {NNL}\")\n",
    "\n",
    "print(\"\\n---- First model Calibration Metrics over Validation (Temp Scal) ----\")\n",
    "ECE, MCE, BRIER, NNL = compute_calibration_metrics(val_new_first_probs, val_labels, apply_softmax=False, bins=15)\n",
    "print(f\"ECE: {ECE}\"); print(f\"MCE: {MCE}\"); print(f\"BRIER: {BRIER}\"); print(f\"NNL: {NNL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- First model Calibration Metrics over Test ----\n",
      "ECE: 2.4884950649738307\n",
      "MCE: 1.3317647820711134\n",
      "BRIER: 0.008238566108047962\n",
      "NNL: 0.19300468266010284\n",
      "\n",
      "---- First model Calibration Metrics over Test (Temp Scal) ----\n",
      "ECE: 0.8139485321938993\n",
      "MCE: 0.1761002254486084\n",
      "BRIER: 0.007907265797257423\n",
      "NNL: 0.17737245559692383\n"
     ]
    }
   ],
   "source": [
    "test_first_model_logits = test_logits_list[0]\n",
    "new_test_first_logits = T1(test_first_model_logits)\n",
    "test_new_first_probs = softmax(new_test_first_logits)\n",
    "test_first_probs = softmax(test_first_model_logits)\n",
    "\n",
    "print(\"---- First model Calibration Metrics over Test ----\")\n",
    "ECE, MCE, BRIER, NNL = compute_calibration_metrics(test_first_probs, test_labels, apply_softmax=False, bins=15)\n",
    "print(f\"ECE: {ECE}\"); print(f\"MCE: {MCE}\"); print(f\"BRIER: {BRIER}\"); print(f\"NNL: {NNL}\")\n",
    "\n",
    "print(\"\\n---- First model Calibration Metrics over Test (Temp Scal) ----\")\n",
    "ECE, MCE, BRIER, NNL = compute_calibration_metrics(test_new_first_probs, test_labels, apply_softmax=False, bins=15)\n",
    "print(f\"ECE: {ECE}\"); print(f\"MCE: {MCE}\"); print(f\"BRIER: {BRIER}\"); print(f\"NNL: {NNL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
